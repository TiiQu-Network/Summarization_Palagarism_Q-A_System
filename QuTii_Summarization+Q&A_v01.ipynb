{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9i0TcVeGDwv",
        "outputId": "932a130c-f4cb-4bfe-be31-bafcdae9b57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.98\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install fitz\n",
        "#Restart runtime after cell runs installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "1tRWBU3GGJfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "id": "LiPHz3LjGJh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown -O  t5_que_gen.zip --id 1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\n",
        "!unzip t5_que_gen.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcjjHSDJJztM",
        "outputId": "e6a625d4-f8d4-4c20-ae36-0b53d8e5fa06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\n",
            "To: /content/t5_que_gen.zip\n",
            "100% 1.65G/1.65G [00:18<00:00, 87.3MB/s]\n",
            "Archive:  t5_que_gen.zip\n",
            "   creating: t5_que_gen_model/\n",
            "   creating: t5_que_gen_model/t5_base_tok_que_gen/\n",
            "  inflating: t5_que_gen_model/t5_base_tok_que_gen/spiece.model  \n",
            " extracting: t5_que_gen_model/t5_base_tok_que_gen/added_tokens.json  \n",
            " extracting: t5_que_gen_model/t5_base_tok_que_gen/tokenizer_config.json  \n",
            "  inflating: t5_que_gen_model/t5_base_tok_que_gen/special_tokens_map.json  \n",
            "   creating: t5_que_gen_model/t5_base_que_gen/\n",
            "  inflating: t5_que_gen_model/t5_base_que_gen/config.json  \n",
            "  inflating: t5_que_gen_model/t5_base_que_gen/pytorch_model.bin  \n",
            " extracting: t5_que_gen_model/logs.zip  \n",
            "   creating: t5_ans_gen_model/\n",
            "   creating: t5_ans_gen_model/t5_base_tok_ans_gen/\n",
            "  inflating: t5_ans_gen_model/t5_base_tok_ans_gen/spiece.model  \n",
            "  inflating: t5_ans_gen_model/t5_base_tok_ans_gen/added_tokens.json  \n",
            " extracting: t5_ans_gen_model/t5_base_tok_ans_gen/tokenizer_config.json  \n",
            "  inflating: t5_ans_gen_model/t5_base_tok_ans_gen/special_tokens_map.json  \n",
            "   creating: t5_ans_gen_model/t5_base_ans_gen/\n",
            "  inflating: t5_ans_gen_model/t5_base_ans_gen/config.json  \n",
            "  inflating: t5_ans_gen_model/t5_base_ans_gen/pytorch_model.bin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9XsSQHNK4M8",
        "outputId": "23108d47-0221-4ade-efd5-94fd3c082727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentencepiece"
      ],
      "metadata": {
        "id": "kT4gSqW5LgcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueGenerator():\n",
        "  def __init__(self):\n",
        "    self.que_model = T5ForConditionalGeneration.from_pretrained('./t5_que_gen_model/t5_base_que_gen/')\n",
        "    self.ans_model = T5ForConditionalGeneration.from_pretrained('./t5_ans_gen_model/t5_base_ans_gen/')\n",
        "\n",
        "    self.que_tokenizer = T5Tokenizer.from_pretrained('./t5_que_gen_model/t5_base_tok_que_gen/')\n",
        "    self.ans_tokenizer = T5Tokenizer.from_pretrained('./t5_ans_gen_model/t5_base_tok_ans_gen/')\n",
        "    \n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    self.que_model = self.que_model.to(self.device)\n",
        "    self.ans_model = self.ans_model.to(self.device)\n",
        "  \n",
        "  def generate(self, text):\n",
        "    answers = self._get_answers(text)\n",
        "    questions = self._get_questions(text, answers)\n",
        "    output = [{'answer': ans, 'question': que} for ans, que in zip(answers, questions)]\n",
        "    return output\n",
        "  \n",
        "  def _get_answers(self, text):\n",
        "    # split into sentences\n",
        "    sents = sent_tokenize(text)\n",
        "\n",
        "    examples = []\n",
        "    for i in range(len(sents)):\n",
        "      input_ = \"\"\n",
        "      for j, sent in enumerate(sents):\n",
        "        if i == j:\n",
        "            sent = \"[HL] %s [HL]\" % sent\n",
        "        input_ = \"%s %s\" % (input_, sent)\n",
        "        input_ = input_.strip()\n",
        "      input_ = input_ + \" </s>\"\n",
        "      examples.append(input_)\n",
        "    \n",
        "    batch = self.ans_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.ans_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
        "                                attention_mask=batch['attention_mask'].to(self.device), \n",
        "                                max_length=32,\n",
        "                                # do_sample=False,\n",
        "                                # num_beams = 4,\n",
        "                                )\n",
        "    dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    answers = [item.split('[SEP]') for item in dec]\n",
        "    answers = chain(*answers)\n",
        "    answers = [ans.strip() for ans in answers if ans != ' ']\n",
        "    return answers\n",
        "  \n",
        "  def _get_questions(self, text, answers):\n",
        "    examples = []\n",
        "    for ans in answers:\n",
        "      input_text = \"%s [SEP] %s </s>\" % (ans, text)\n",
        "      examples.append(input_text)\n",
        "    \n",
        "    batch = self.que_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.que_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
        "                                attention_mask=batch['attention_mask'].to(self.device), \n",
        "                                max_length=32,\n",
        "                                num_beams = 4)\n",
        "    dec = [self.que_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    return dec"
      ],
      "metadata": {
        "id": "tNS7Z9khK4Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"/content/1-s2.0-S004896972208367X-main-2-9.pdf\""
      ],
      "metadata": {
        "id": "HKcnTgkrK4Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do not run code from here"
      ],
      "metadata": {
        "id": "GtiwlEjrZwcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_summary = pipeline(\"summarization\", model=model, tokenizer=tokenizer, max_length=80)\n",
        "que_generator = QueGenerator()\n",
        "output_summary = pipe_summary(text)\n",
        "output_summary = output_summary[0][\"summary_text\"]\n",
        "que_generator.generate(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff0Pc-vzHW_U",
        "outputId": "ef2b7882-513a-45d9-963b-6e894cf86244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but you input_length is only 75. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': '<pad>integration or retention of trees',\n",
              "  'question': '<pad> What does agroforestry involve?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s>',\n",
              "  'question': '<pad> What is agroforestry?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '<pad>Ecological interactions',\n",
              "  'question': \"<pad> What can benefit smallholder farmers' crop yields while minimising the need for farming inputs?</s>\"},\n",
              " {'answer': '</s><pad><pad>',\n",
              "  'question': '<pad> What is the purpose of agroforestry?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\\n\",text)\n",
        "print(\"Paraphrased Text:\\n\",output_summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8unDcwlHXD0",
        "outputId": "cfeca10e-86f8-433e-ed86-7c2ffe3884d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Agroforestry involves the integration or retention of trees in agricultural landscapes for socio-economic and ecological benefit (Schroth et al., 2004).Ecological interactions between trees, soils and crops in agroforests can benefit smallholder farmers' crop yields whilst minimising the need for farming inputs (Jose, 2009; Ajayi et al., 2011). \n",
            "Paraphrased Text:\n",
            "  Agroforestry involves the integration or retention of trees in agricultural landscapes for socio-economic and ecological benefit (Schroth et al., 2004). Ecological interactions between trees, soils and crops in agroforests can benefit smallholder farmers' crop yields whilst minimising the need for farming inputs .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_1 = nltk.word_tokenize(text)\n",
        "words_2 = nltk.word_tokenize(output_summary)\n",
        "print(len(words_1), len(words_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDLRR_K3UIyi",
        "outputId": "a3c0de43-c7da-4926-fefb-2c97a8effe96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code not to run end"
      ],
      "metadata": {
        "id": "uw1jgCgjZ2zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI45hdFM9AtI",
        "outputId": "4672de55-2cd3-40ac-aee1-f3e58bf839d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.22.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "\n",
        "def get_text_data(pdf_path):\n",
        "  with fitz.open(pdf_path) as doc:\n",
        "      text = \"\"\n",
        "      for page in doc:\n",
        "          text += page.get_text()\n",
        "\n",
        "      return text"
      ],
      "metadata": {
        "id": "gdPXto4M8V0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  #NLTK Library is for sentence and word tokenization.\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2xTH-6y8V3Y",
        "outputId": "af0ab8e0-0aab-4f28-f073-c9f5761915a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = get_text_data('/content/test_main.pdf')\n",
        "\n",
        "result = tokenize.sent_tokenize(text)\n",
        "\n",
        "str_paragraph = ''\n",
        "paragraphs = []\n",
        "\n",
        "for i in range(len(result)):\n",
        "\n",
        "  sentence = result[i]\n",
        "  len_para = len(tokenize.word_tokenize(str_paragraph))\n",
        "\n",
        "  if len_para < 200:\n",
        "    str_paragraph = str_paragraph + ' ' + sentence\n",
        "\n",
        "  elif len_para >= 200 :\n",
        "    paragraphs.append(str_paragraph)\n",
        "\n",
        "    str_paragraph = ''\n",
        "    str_paragraph = str_paragraph + ' ' + sentence\n",
        "\n",
        "  elif i == len(result) - 1:\n",
        "    paragraphs.append(str_paragraph)"
      ],
      "metadata": {
        "id": "PHzj-RXB8qPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "9NAC1edw-qmg",
        "outputId": "18a7847c-2e86-4a9b-ccba-a5c57afaa926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A multi-model ensemble is constructed with the global climate models \\nthat are found to best reproduce the wind climate in Northern Europe. The results anticipate an overall decline in \\nwind power density, especially in the high-emissions scenario and in certain regions (up to 30% off Western \\nIreland), which should be taken into account in planning future offshore wind deployments. As an exception, \\nslight increases (around 10%) are projected in certain areas of the Baltic Sea. The general decline is less pro-\\nnounced in the low-emissions scenario. Indeed, the results prove that reducing emissions as advocated by current \\nclimate objectives would not only weaken the declining trend but also lead to a more stable resource. 1. Introduction \\nCutting carbon emissions to mitigate climate change and reducing \\nthe over-dependence on fossil fuels are listed as a priority in a great \\nnumber of countries throughout the world. As a result, investments in \\nthe energy sector are shifting towards renewable energies; among all the \\nrenewables, wind energy is experiencing remarkable growth in recent \\nyears. Globally, the wind energy sector has undergone two record years \\nin new capacity installations – a total of 93 GW in 2020 [1] and 94 GW in \\n2021 [2].'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "ISTHK-ojbSc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_summary = pipeline(\"summarization\", model=model, tokenizer=tokenizer, truncation=True)\n",
        "que_generator = QueGenerator()\n",
        "output_summary = pipe_summary(paragraphs[1])\n",
        "output_summary = output_summary[0][\"summary_text\"]\n",
        "que_generator.generate(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbuhw-D7-s0o",
        "outputId": "70bd0ee8-8141-4cfc-c162-97bc6b6b2348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': '<pad>Northern Europe',\n",
              "  'question': '<pad> Where is the wind climate found to be best reproduced?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s><pad><pad><pad><pad>',\n",
              "  'question': '<pad> What type of wind power density is predicted to decline in some areas of the Baltic Sea?</s><pad><pad><pad><pad>'},\n",
              " {'answer': '<pad>high-emissions scenario',\n",
              "  'question': '<pad> The results anticipate an overall decline in wind power density especially in what scenario?</s><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s>',\n",
              "  'question': '<pad> What ensemble is constructed with the global climate models that are found to best reproduce the wind climate in Northern Europe?</s>'},\n",
              " {'answer': '<pad>around 10',\n",
              "  'question': '<pad> How many light increases are projected in certain areas of the Baltic Sea?</s><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s><pad><pad><pad><pad>',\n",
              "  'question': '<pad> What type of wind power density is predicted to decline in some areas of the Baltic Sea?</s><pad><pad><pad><pad>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "que_generator.generate(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlZR0x-ZBLiP",
        "outputId": "75f6db8b-de72-4768-97c0-66a266a7509b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': '<pad>warmer',\n",
              "  'question': '<pad> What do climate projections for tropical regions forecast?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
              "  'question': '<pad> What is the name of the game?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '<pad>NE South America, Central America,. Central America',\n",
              "  'question': '<pad> Which regions may become drier by 2100?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s>',\n",
              "  'question': \"<pad> What are the IPCC's recent multi-model mean projections?</s><pad><pad><pad><pad><pad><pad><pad>\"},\n",
              " {'answer': '<pad>Central America',\n",
              "  'question': '<pad> Along with Southern Africa and SE Asia, what tropical region is predicted to become wetter by 2100?</s>'},\n",
              " {'answer': '</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
              "  'question': '<pad> What is the name of the item that could cause the temperature to rise in the tropics?</s><pad><pad>'},\n",
              " {'answer': '<pad>2100',\n",
              "  'question': '<pad> By what year will other tropical regions become wetter?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'},\n",
              " {'answer': '</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
              "  'question': '<pad> What is the name of the item that could cause the temperature to rise in the tropics?</s><pad><pad>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\\n\",paragraphs[1])\n",
        "print(\"Paraphrased Text:\\n\",output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY1y7ABGAQpH",
        "outputId": "fbc1b1f1-8e56-40cf-beec-7996ee5852f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "  A multi-model ensemble is constructed with the global climate models \n",
            "that are found to best reproduce the wind climate in Northern Europe. The results anticipate an overall decline in \n",
            "wind power density, especially in the high-emissions scenario and in certain regions (up to 30% off Western \n",
            "Ireland), which should be taken into account in planning future offshore wind deployments. As an exception, \n",
            "slight increases (around 10%) are projected in certain areas of the Baltic Sea. The general decline is less pro-\n",
            "nounced in the low-emissions scenario. Indeed, the results prove that reducing emissions as advocated by current \n",
            "climate objectives would not only weaken the declining trend but also lead to a more stable resource. 1. Introduction \n",
            "Cutting carbon emissions to mitigate climate change and reducing \n",
            "the over-dependence on fossil fuels are listed as a priority in a great \n",
            "number of countries throughout the world. As a result, investments in \n",
            "the energy sector are shifting towards renewable energies; among all the \n",
            "renewables, wind energy is experiencing remarkable growth in recent \n",
            "years. Globally, the wind energy sector has undergone two record years \n",
            "in new capacity installations – a total of 93 GW in 2020 [1] and 94 GW in \n",
            "2021 [2].\n",
            "Paraphrased Text:\n",
            "  A multi-model ensemble is constructed with the global climate models that are found to best reproduce the wind climate in Northern Europe . The results anticipate an overall decline in wind power density, especially in the high-emissions scenario . As an exception,  light increases (around 10%) are projected in certain areas of the Baltic Sea .\n"
          ]
        }
      ]
    }
  ]
}